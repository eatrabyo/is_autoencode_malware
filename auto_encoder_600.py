import numpy as np
import tensorflow as tf
import keras
import random
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
import wandb
from wandb.keras import WandbMetricsLogger

from scaler import scaler

config = {
        "layer_1": 2300,
        "layer_2": 2000,
        "layer_3": 1700,
        "layer_4": 1400,
        "layer_5": 1100,
        "layer_6": 800,
        "layer_7": 600,
        "origin": 2381,
        "relu": "relu",
        "sigmoid": "sigmoid",
        "leaky_relu":keras.layers.LeakyReLU(),
        "optimizer": keras.optimizers.Adam(learning_rate=0.0005),
        "loss": keras.losses.MeanAbsoluteError(),
        "metric": keras.metrics.MeanAbsoluteError(),
        "epoch": 100,
        "batch_size": 512,
        "min_lr": 0.00005,
        "reduce_lr_patience": 10
    }

# Clear all previously registered custom objects
keras.saving.get_custom_objects().clear()
@keras.saving.register_keras_serializable()
class DeepAE(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(DeepAE,self).__init__(**kwargs)
        
        self.encoder = keras.Sequential([
            keras.layers.Dense(config['layer_1'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_2'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_3'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_4'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_5'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_6'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_7'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            
        ])

        self.decoder = keras.Sequential([
            keras.layers.Dense(config['layer_6'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_5'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_4'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_3'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_2'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_1'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['origin'], activation = 'linear'),
            keras.layers.BatchNormalization(),
        ])

    def call(self,x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
    
    def get_config(self):
        config = super().get_config()
        return config
    
def get_ae():
    # model
    autoencoder = DeepAE()

    autoencoder.compile(optimizer=config['optimizer'], loss=config['loss'])

    return autoencoder

def classifier():
    model = keras.Sequential([
        keras.layers.Dense(400, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(300, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(200, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(100, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(50, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(25, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(12, activation=keras.layers.LeakyReLU()),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=keras.losses.BinaryCrossentropy(),
                  metrics = [
                      keras.metrics.BinaryAccuracy(name = 'Binary Accuracy',threshold=0.5),
                      ])
    return model

if __name__ == '__main__':
    run = wandb.init(project="autoencoder_malware",group='ae_600', config=config)
    print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
    # seed = 123
    # random.seed(seed)
    # tf.random.set_seed(seed)
    # np.random.seed(seed)

    TRAIN_PATH = 'data/train_set.npz'
    VALID_PATH = 'data/validate_set.npz'
    TEST_PATH = 'data/test_set.npz'

    train_set = np.load(TRAIN_PATH)
    valid_set = np.load(VALID_PATH)
    test_set = np.load(TEST_PATH)

    X_train = train_set['x']
    y_train = train_set['y']

    X_valid = valid_set['x']
    y_valid = valid_set['y']

    X_test = test_set['x']
    y_test = test_set['y']

    print(f'X,y dtype\nTrain: {X_train.dtype},{y_train.dtype}\nValidate: {X_valid.dtype},{y_valid.dtype}\nTest: {X_test.dtype},{y_test.dtype}')

    X_train_preprocess, X_valid_preprocess, X_test_preprocess = scaler(X_train,X_test,X_valid)

    # convert to tensor
    X_train_preprocess = tf.convert_to_tensor(X_train_preprocess,dtype=tf.float32)
    y_train = tf.convert_to_tensor(y_train,dtype=tf.int32)
    
    X_valid_preprocess = tf.convert_to_tensor(X_valid_preprocess,dtype=tf.float32)
    y_valid = tf.convert_to_tensor(y_valid,dtype=tf.int32)

    X_test_preprocess = tf.convert_to_tensor(X_test_preprocess,dtype=tf.float32)
    y_test = tf.convert_to_tensor(y_test,dtype=tf.int32)

    # autoencoder model
    autoencoder = get_ae()

    # callback for AE
    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=config['reduce_lr_patience'], min_lr=config['min_lr'],verbose=1)
    
    callback_list = [reduce_lr, WandbMetricsLogger(log_freq='epoch')]
    
    history = autoencoder.fit(X_train_preprocess, X_train_preprocess,
            epochs = config['epoch'],
            batch_size = config['batch_size'],
            validation_data = (X_valid_preprocess , X_valid_preprocess ),
            shuffle = True, verbose = 2, callbacks = callback_list
            )
    
    plt.figure(figsize=(12,6))
    plt.subplot(1,2,1)
    plt.plot(history.history["loss"], label="Training Loss")
    plt.plot(history.history["val_loss"], label="Validation Loss")
    plt.legend()
    plt.title('Training and Validation MAE Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('MAE')
    # plt.show()

    test_result = autoencoder.evaluate(X_test_preprocess,X_test_preprocess)
    print(f'{autoencoder.metrics_names}: {test_result}')
    wandb.log({'test_mae':test_result})
    autoencoder.save('model/ae_600.keras')
    wandb.save('model/ae_600.keras')
    run.finish()

    """
    classifier w&b log start here
    """
    run = wandb.init(project="classify_malware",group='ae_600')

    autoencoder = keras.models.load_model('model/ae_600.keras',custom_objects={"DeepAE": DeepAE})

    encoded_train = autoencoder.encoder(X_train_preprocess)
    encoded_valid = autoencoder.encoder(X_valid_preprocess)
    encoded_test = autoencoder.encoder(X_test_preprocess)

    model = classifier()

    # callback for classifier
    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=15, min_lr=0.00005,verbose=1)

    callback_list = [reduce_lr, WandbMetricsLogger(log_freq='epoch')]

    history = model.fit(encoded_train, y_train,
            epochs = 50,
            batch_size = 512,
            validation_data = (encoded_valid , y_valid ),
            shuffle = True, verbose = 2, callbacks = callback_list
            )
    
    precision = keras.metrics.Precision(name = 'Precision')
    recall = keras.metrics.Recall(name = 'Recall')
    false_neg = keras.metrics.FalseNegatives(name = 'FalseNegative')

    predict = model.predict(encoded_test)

    test_result = model.evaluate(encoded_test,y_test)

    precision.update_state(y_pred=predict,y_true=y_test)
    recall.update_state(y_pred=predict,y_true=y_test)
    false_neg.update_state(y_pred=predict,y_true=y_test)

    print(f'{model.metrics_names}: {test_result}')

    wandb.log({'test_bi_cross_entropy':test_result[0],
            'binary_accuracy':test_result[1],
            'precision':precision.result(),
            'recall':recall.result(),
            'f1':f1_score(y_test, np.where(predict > 0.5, 1,0), average='binary'),
            'false_negative_rate':false_neg.result()
            })
    run.finish()
    model.save('model/ae_600_classifier.keras')