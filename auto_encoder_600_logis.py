import numpy as np
import tensorflow as tf
import keras
import random
import matplotlib.pyplot as plt
import wandb
from wandb.keras import WandbMetricsLogger
from sklearn.metrics import f1_score
from sklearn.linear_model import LogisticRegression
from joblib import dump, load

from scaler import scaler

config = {
        "layer_1": 2300,
        "layer_2": 2000,
        "layer_3": 1700,
        "layer_4": 1400,
        "layer_5": 1100,
        "layer_6": 800,
        "layer_7": 600,
        "origin": 2381,
        "relu": "relu",
        "sigmoid": "sigmoid",
        "leaky_relu":keras.layers.LeakyReLU(),
        "optimizer": keras.optimizers.Adam(learning_rate=0.0005),
        "loss": keras.losses.MeanAbsoluteError(),
        "metric": keras.metrics.MeanAbsoluteError(),
        "epoch": 100,
        "batch_size": 512,
        "min_lr": 0.00005,
        "reduce_lr_patience": 10
    }

# Clear all previously registered custom objects
keras.saving.get_custom_objects().clear()
@keras.saving.register_keras_serializable()
class DeepAE(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(DeepAE,self).__init__(**kwargs)
        
        self.encoder = keras.Sequential([
            keras.layers.Dense(config['layer_1'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_2'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_3'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_4'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_5'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_6'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_7'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            
        ])

        self.decoder = keras.Sequential([
            keras.layers.Dense(config['layer_6'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_5'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_4'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_3'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_2'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['layer_1'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(config['origin'], activation = 'linear'),
            keras.layers.BatchNormalization(),
        ])

    def call(self,x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
    
    def get_config(self):
        config = super().get_config()
        return config
    
def get_ae():
    # model
    autoencoder = DeepAE()

    autoencoder.compile(optimizer=config['optimizer'], loss=config['loss'])

    return autoencoder

if __name__ == '__main__':
        run = wandb.init(project="classify_malware",group='600_logis')
        print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

        # seed = 123
        # random.seed(seed)
        # tf.random.set_seed(seed)
        # np.random.seed(seed)

        TRAIN_PATH = 'data/train_set.npz'
        VALID_PATH = 'data/validate_set.npz'
        TEST_PATH = 'data/test_set.npz'

        train_set = np.load(TRAIN_PATH)
        valid_set = np.load(VALID_PATH)
        test_set = np.load(TEST_PATH)

        X_train = train_set['x']
        y_train = train_set['y']

        X_valid = valid_set['x']
        y_valid = valid_set['y']

        X_test = test_set['x']
        y_test = test_set['y']

        X_train_preprocess, X_valid_preprocess, X_test_preprocess = scaler(X_train,X_test,X_valid)

        # convert to tensor
        X_train_preprocess = tf.convert_to_tensor(X_train_preprocess,dtype=tf.float32)
        y_train = tf.convert_to_tensor(y_train,dtype=tf.int32)
        y_train = tf.reshape(y_train,[tf.shape(y_train)[0],1])

        X_valid_preprocess = tf.convert_to_tensor(X_valid_preprocess,dtype=tf.float32)
        y_valid = tf.convert_to_tensor(y_valid,dtype=tf.int32)
        y_valid = tf.reshape(y_valid,[tf.shape(y_valid)[0],1])

        X_test_preprocess = tf.convert_to_tensor(X_test_preprocess,dtype=tf.float32)
        y_test = tf.convert_to_tensor(y_test,dtype=tf.int32)
        y_test = tf.reshape(y_test,[tf.shape(y_test)[0],1])


        autoencoder = get_ae()

        # callback for AE
        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=config['reduce_lr_patience'], min_lr=config['min_lr'],verbose=1)
        
        callback_list = [reduce_lr, WandbMetricsLogger(log_freq='epoch')]
        
        history = autoencoder.fit(X_train_preprocess, X_train_preprocess,
                epochs = config['epoch'],
                batch_size = config['batch_size'],
                validation_data = (X_valid_preprocess , X_valid_preprocess ),
                shuffle = True, verbose = 2, callbacks = callback_list
                )
        
        plt.figure(figsize=(12,6))
        plt.subplot(1,2,1)
        plt.plot(history.history["loss"], label="Training Loss")
        plt.plot(history.history["val_loss"], label="Validation Loss")
        plt.legend()
        plt.title('Training and Validation MAE Over Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('MAE')
        # plt.show()

        test_result = autoencoder.evaluate(X_test_preprocess,X_test_preprocess)
        print(f'{autoencoder.metrics_names}: {test_result}')

        encoded_train = autoencoder.encoder(X_train_preprocess)
        encoded_valid = autoencoder.encoder(X_valid_preprocess)
        encoded_test = autoencoder.encoder(X_test_preprocess)

        model = LogisticRegression(solver = 'newton-cg', n_jobs = -1)

        accuracy = keras.metrics.BinaryAccuracy(name = 'Binary Accuracy',threshold=0.5)
        precision = keras.metrics.Precision(name = 'Precision')
        recall = keras.metrics.Recall(name = 'Recall')
        false_neg = keras.metrics.FalseNegatives(name = 'FalseNegative')

        model.fit(encoded_train,y_train)
        predict = model.predict(encoded_test)

        accuracy.update_state(y_pred=predict,y_true=y_test)
        precision.update_state(y_pred=predict,y_true=y_test)
        recall.update_state(y_pred=predict,y_true=y_test)
        false_neg.update_state(y_pred=predict,y_true=y_test)

        wandb.log({
                'binary_accuracy':accuracy.result(),
                'precision':precision.result(),
                'recall':recall.result(),
                'f1':f1_score(y_test, np.where(predict > 0.5, 1,0), average='binary'),
                'false_negative_rate':false_neg.result()
                })
        run.finish()
        dump(model, 'model/ae_600_logis.joblib')