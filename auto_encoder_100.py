import numpy as np
import tensorflow as tf
import keras
import random
import matplotlib.pyplot as plt
import wandb
from wandb.keras import WandbMetricsLogger

from scaler import scaler

config = {
        "layer_1": 2000,
        "layer_2": 1800,
        "layer_3": 1600,
        "layer_4": 1400,
        "layer_5": 1200,
        "layer_6": 1000,
        "layer_7": 800,
        "layer_8": 600,
        "layer_9": 400,
        "layer_10": 200,
        "layer_11": 100,
        "origin": 2381,
        "relu": "relu",
        "sigmoid": "sigmoid",
        "leaky_relu":keras.layers.LeakyReLU(),
        "optimizer": keras.optimizers.Adam(learning_rate=0.0001),
        "loss": keras.losses.MeanAbsoluteError(),
        "metric": keras.metrics.MeanAbsoluteError(),
        "epoch": 100,
        "batch_size": 512,
        "min_lr": 0.00005,
        "reduce_lr_patience": 20
    }

# Clear all previously registered custom objects
keras.saving.get_custom_objects().clear()
@keras.saving.register_keras_serializable()
class DeepAE(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(DeepAE,self).__init__(**kwargs)
        
        self.encoder = keras.Sequential([
            keras.layers.Dense(config['layer_1'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_2'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_3'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_4'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(config['layer_5'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_6'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_7'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_8'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(config['layer_9'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_10'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_11'], activation = config['leaky_relu'])
        ])

        self.decoder = keras.Sequential([
            keras.layers.Dense(config['layer_10'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_9'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_8'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_7'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_6'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_5'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(config['layer_4'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_3'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_2'], activation = config['leaky_relu']),
            keras.layers.Dense(config['layer_1'], activation = config['leaky_relu']),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(config['origin'], activation = 'linear')
        ])

    def call(self,x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
    
    def get_config(self):
        config = super().get_config()
        return config
    
def get_model():
    # model
    autoencoder = DeepAE()

    autoencoder.compile(optimizer=config['optimizer'], loss=config['loss'])

    return autoencoder
    

if __name__ == '__main__':
    run = wandb.init(project="autoencoder_malware",group='ae_100', config=config)
    print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
    # seed = 123
    # random.seed(seed)
    # tf.random.set_seed(seed)
    # np.random.seed(seed)

    TRAIN_PATH = 'data/train_set.npz'
    VALID_PATH = 'data/validate_set.npz'
    TEST_PATH = 'data/test_set.npz'

    train_set = np.load(TRAIN_PATH)
    valid_set = np.load(VALID_PATH)
    test_set = np.load(TEST_PATH)

    X_train = train_set['x']
    y_train = train_set['y']

    X_valid = valid_set['x']
    y_valid = valid_set['y']

    X_test = test_set['x']
    y_test = test_set['y']

    print(f'X,y dtype\nTrain: {X_train.dtype},{y_train.dtype}\nValidate: {X_valid.dtype},{y_valid.dtype}\nTest: {X_test.dtype},{y_test.dtype}')

    X_train_preprocess, X_valid_preprocess, X_test_preprocess = scaler(X_train,X_test,X_valid)

    # convert to tensor
    X_train_preprocess = tf.convert_to_tensor(X_train_preprocess,dtype=tf.float32)
    y_train = tf.convert_to_tensor(y_train,dtype=tf.int32)
    
    X_valid_preprocess = tf.convert_to_tensor(X_valid_preprocess,dtype=tf.float32)
    y_valid = tf.convert_to_tensor(y_valid,dtype=tf.int32)

    X_test_preprocess = tf.convert_to_tensor(X_test_preprocess,dtype=tf.float32)
    y_test = tf.convert_to_tensor(y_test,dtype=tf.int32)

    # model
    autoencoder = get_model()

    # callback
    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=config['reduce_lr_patience'], min_lr=config['min_lr'],verbose=1)
    
    callback_list = [reduce_lr, WandbMetricsLogger(log_freq='epoch')]
    
    history = autoencoder.fit(X_train_preprocess, X_train_preprocess,
            epochs = config['epoch'],
            batch_size = config['batch_size'],
            validation_data = (X_valid_preprocess , X_valid_preprocess ),
            shuffle = True, verbose = 2, callbacks = callback_list
            )
    
    plt.figure(figsize=(12,6))
    plt.subplot(1,2,1)
    plt.plot(history.history["loss"], label="Training Loss")
    plt.plot(history.history["val_loss"], label="Validation Loss")
    plt.legend()
    plt.title('Training and Validation MAE Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('MAE')

    # plt.subplot(1,2,2)
    # plt.plot(history.history['mean_absolute_error'],label='Training MAE')
    # plt.plot(history.history['val_mean_absolute_error'],label='Validation MAE')

    # plt.plot(history.history['mean_squared_error'],label='Training MSE')
    # plt.plot(history.history['val_mean_squared_error'],label='Validation MSE')

    # plt.legend()
    # plt.title('Training and Validation MAE & MSE Over Epochs')
    # plt.xlabel('Epochs')

    # plt.show()

    test_result = autoencoder.evaluate(X_test_preprocess,X_test_preprocess)
    print(f'{autoencoder.metrics_names}: {test_result}')
    wandb.log({'test_mae':test_result})
    autoencoder.save('model/ae_100.keras')
    wandb.save('model/ae_100.keras')
    run.finish()

